{
  "microsoft/DialoGPT-medium": {
    "status": "error",
    "output": "Loading microsoft/DialoGPT-medium...\nModel loaded in 33.12s\nParameters: 354,823,168\nMemory usage: 2624.1MB\n",
    "error": "2025-07-09 19:27:04.267714: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-07-09 19:27:06.142922: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\nTraceback (most recent call last):\n  File \"<string>\", line 24, in <module>\n  File \"C:\\Users\\mateo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2877, in __call__\n    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\mateo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2987, in _call_one\n    return self.encode_plus(\n           ^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\mateo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 3054, in encode_plus\n    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\mateo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2779, in _get_padding_truncation_strategies\n    raise ValueError(\nValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.\n"
  },
  "gpt2-medium": {
    "status": "error",
    "output": "Loading gpt2-medium...\nModel loaded in 65.78s\nParameters: 354,823,168\nMemory usage: 2189.0MB\n",
    "error": "2025-07-09 19:28:10.295636: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-07-09 19:28:11.771591: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\nTraceback (most recent call last):\n  File \"<string>\", line 24, in <module>\n  File \"C:\\Users\\mateo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2877, in __call__\n    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\mateo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2987, in _call_one\n    return self.encode_plus(\n           ^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\mateo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 3054, in encode_plus\n    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\mateo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2779, in _get_padding_truncation_strategies\n    raise ValueError(\nValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.\n"
  },
  "bert-large-uncased": {
    "status": "success",
    "output": "Loading bert-large-uncased...\nModel loaded in 45.59s\nParameters: 335,141,888\nMemory usage: 811.2MB\nOutput shape: torch.Size([1, 14, 1024])\nLarge model test completed successfully\n",
    "error": null
  }
}